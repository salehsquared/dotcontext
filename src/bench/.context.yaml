version: 1
last_updated: 2026-02-13T06:14:48.428Z
fingerprint: 80c2bdd0
scope: src/bench
summary: >
  The `src/bench` directory implements benchmark generation, execution, and scoring for measuring whether
  `.context.yaml` improves LLM performance versus baseline prompts. It builds tasks from static code and git signals,
  runs scoped baseline/context prompts, and reports quality and token-efficiency metrics.
maintenance: |-
  If you modify files in this directory, update this .context.yaml to reflect
  your changes. Update the summary, and any decisions or constraints that changed.
  Do NOT update the fingerprint manually â€” run `context rehash` or it will be
  updated automatically on the next `context status` check.
  If you only read files in this directory, do not modify this file.
  Do not include secrets, API keys, passwords, or PII in this file.
decisions:
  - what: The directory utilizes a specific scoring system for task evaluation.
    why: This allows for consistent and structured results across varying task types.
    tradeoff: The complexity of managing multiple scoring methods may introduce overhead in task definition.
  - what: Token estimation is computed from the actual benchmark prompts at runtime.
    why: This keeps token metrics aligned with what the model actually receives, including judge prompts.
    tradeoff: Estimates are still approximate unless provider usage metadata is exposed.
  - what: Benchmark prompts are scoped to the task source directory window (root/parent/scope/children).
    why: This better matches realistic retrieval behavior and avoids full-project prompt inflation.
    tradeoff: Very cross-cutting questions may need broader context than the default scope window.
constraints:
  - Dependencies must be synchronized with the task definitions and given the correct criteria for evaluation.
  - Git operations must handle failures gracefully to avoid breaking the benchmarking process.
  - Scoped prompt construction must preserve deterministic ordering for stable benchmark comparisons.
dependencies:
  internal:
    - ../core/scanner.js
    - ../core/schema.js
    - ../generator/dependencies.js
    - ../generator/static.js
    - ../providers/index.js
    - ./git.js
    - ./ground-truth.js
    - ./prompts.js
    - ./scorer.js
    - ./token-estimator.js
    - ./types.js
exports:
  - "function isGitRepo(dirPath: string): boolean"
  - "function getFixCommits(repoPath: string, count: number): Commit[]"
  - "function getFeatureCommits(repoPath: string, count: number): Commit[]"
  - "async function cloneRepo( url: string, destDir: string, depth = 100, ): Promise<string>"
  - "async function buildDepSets( scanResult: ScanResult, ): Promise<"
  - "async function buildReverseDeps( scanResult: ScanResult, ): Promise<Map<string, string[]>>"
  - "async function buildDirFacts( scanResult: ScanResult, ): Promise<Map<string, DirFacts>>"
  - 'function buildFileTree(scanResult: ScanResult, indent = ""): string'
  - "function computeScopeWindow( sourceScope: string, scanResult: ScanResult, ): ScopeWindow"
  - "function buildScopedFileTree( sourceScope: string, scanResult: ScanResult, ):"
  - BENCH_SYSTEM_PROMPT
  - "function buildReadmeSnippet(readme: string | null, maxChars?: number): string | null"
  - "function buildBaselinePrompt( fileTree: string, readme: string | null, question: string, sourceScope?: string, ):
    string"
  - "function buildContextPrompt( fileTree: string, contextFiles: Map<string, ContextFile>, question: string,
    sourceScope?: string, ): string"
  - "function buildJudgePrompt( question: string, response: string, referenceFacts: string[], ): string"
  - interface DefaultRepo
  - "DEFAULT_REPOS: DefaultRepo[]"
  - "async function cloneAndInit( repo: DefaultRepo, initFn: (rootPath: string) => Promise<void>, ): Promise<string>"
  - "async function cleanupRepos(tempDirs: string[]): Promise<void>"
  - interface RunBenchOptions
  - "async function runBench(options: RunBenchOptions): Promise<TaskResult[]>"
  - JUDGE_SYSTEM_PROMPT
  - "function detectAbstention(response: string): boolean"
  - "function scoreListCoverage(response: string, expected: string[]): number"
  - "function scoreTopkRecall(response: string, expected: string[]): number"
  - "function scoreTargetHit(response: string, expected: string[]): number"
  - "function scoreMrr(response: string, expected: string[]): number"
  - "function scoreFileSetF1(response: string, expected: string[]): number"
  - "function resolveTokenProfile(provider: string, model: string):"
  - "function estimateInputTokens(options:"
derived_fields:
  - version
  - last_updated
  - fingerprint
  - scope
  - dependencies.internal
  - exports
